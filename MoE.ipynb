{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Mixture of Experts and Switch Transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Overview</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After explaining what a language model is and how we can build one in a <a href='https://github.com/lsafarne/NLPBites.github.io/blob/main/Language_Model.ipynb'>previous post</a>, in this post, I will delve into Switch Transformers, a more advanced topic in building NLP models, and demonstrate how to modify the model we previously built to incorporate Switch Transformers. Before discussing Switch Transformers in detail, I will explain the idea of a 'Mixture of Experts' and 'gating networks,' the foundational principles upon which Switch Transformers are built.\n",
    "\n",
    "The Mixture of Experts (MoEs) approach proposes the idea of utilizing an ensemle of specialized models, where each model excels in a particular domain. For language modeling, this means employing multiple models, each adept at capturing specific linguistic nuances.\n",
    "\n",
    "Switch Transformers, proposed in <a href=https://arxiv.org/abs/2101.03961>this paper</a>, a variant of the MoEs approach, offer a distinctive modification to the traditional transformer architecture. By introducing efficiencies and optimizations, they further enhance the overall performance of models.\" Using Switch Transformers, a language model can be scaled up substantially, resulting in enhancing the overall performance of models.\n",
    "\n",
    "In the upcoming sections, I'll begin by introducing the concept of MoEs. Next, I'll adjust the model I presented in a <a href='https://github.com/lsafarne/NLPBites.github.io/blob/main/Language_Model.ipynb'>previous post</a>, where I explained the basics of language models. Subsequently, I'll discuss switch transformers and conclude by integrating them into our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mixure of Experts (MoEs)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of \"Mixture of Experts\" (MoE) is an approach in machine learning where multiple specialized components (or \"experts\") come together to make a collective decision.\n",
    "\n",
    "In a MoE model, there are multiple expert networks, and each one is responsible for handling a specific subset or type of data. Experts are individual models or subnetworks, each trained to specialize in a different aspect of the data. For example, in a language modeling task, one expert might specialize in grammar, another in vocabulary usage, another in capturing sentiment, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Advantages</h3>\n",
    "<ul>\n",
    "    <li><b>Specialization:</b> Each expert can become highly specialized in a specific subset of the data, leading to more tailored and accurate predictions.</li>\n",
    "    <li><b>Scalability:</b> Instead of growing a single massive network, adding more experts can increase capacity while maintaining efficiency.</li>\n",
    "    <li><b>Reduced Overfitting:</b> Individual experts are typically smaller networks, making them less prone to overfitting.</li>\n",
    "    <li><b>Flexibility:</b> MoE offers flexibility in terms of architecture. Experts can have different architectures or even be different types of models altogether.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the <a href='https://github.com/lsafarne/NLPBites.github.io/blob/main/Language_Model.ipynb'>previous post</a>, I explained how to implement a language model from scratch. In what follows, I will modify that model to incorporate MoEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Define an Expert</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Define the Mixure of Experts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, num_experts, input_size, hidden_size, output_size):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_size, hidden_size, output_size) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Here we need logic to decide which expert to use for each input\n",
    "        # For simplicity, I choose an expert randomly\n",
    "        selected_expert = torch.randint(len(self.experts), (1,)).item()\n",
    "        return self.experts[selected_expert](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 3: Create the Language Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_experts):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Transformer layer\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        #MoE\n",
    "        self.moe = MixtureOfExperts(num_experts, hidden_size, hidden_size, output_size)\n",
    "    \n",
    "        #fully connected network\n",
    "        self.fc = nn.Linear(output_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output = self.transformer_encoder(embedded)\n",
    "        output = self.moe(output)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gating Networks:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model described above, an expert is randomly selected to process the output of the encoder layer for a given token representation. This raises the question: can we optimize the selection of experts beyond mere randomness? The answer lies in utilizing gating networks. Alongside the experts, a gating network can be used to determine which expert (or combination of experts) should be utilized for a given input. In other words, the gating network determine the weighting of each expert's output based on the input. This way, the model can learn to rely on different experts for different portions of the input space.\n",
    "\n",
    "The gating network is responsible for combining the outputs of the individual experts. It takes the same input as the experts and outputs a set of weights that determine how much each expert's prediction should contribute to the final prediction.\n",
    "\n",
    "The idea is that for a given input, some experts may be more relevant than others, so the gating network <span style='background-color:yellow'>\"routes\"</span> the input to the appropriate experts by assigning higher weights to them.\n",
    "\n",
    "For a given input, the gating network outputs a weight for each expert. These weights determine how much each expert contributes to the final output.\n",
    "The expert networks process the input independently and produce their outputs.\n",
    "The final output is a weighted sum of the experts' outputs based on the gating network's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementation:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExpertsGatingNet(nn.Module):\n",
    "    def __init__(self, num_experts, input_size, hidden_size, output_size):\n",
    "        super(MixtureOfExpertsGatingNet, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_size, hidden_size, output_size) for _ in range(num_experts)])\n",
    "        \n",
    "        # Gating network is a simple feedforward network with softmax output\n",
    "        self.gating = nn.Sequential(\n",
    "            nn.Linear(input_size, num_experts),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.gating(x) # Weights for each expert\n",
    "        \n",
    "        # Compute expert outputs and combine them based on the weights\n",
    "        output = torch.zeros_like(x)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_output = expert(x)\n",
    "            output += weights[:, i].unsqueeze(1) * expert_output\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Note</h3>\n",
    "\n",
    "The nn.Linear layer will transform the input data into a shape of [batch_size, num_experts]. By specifying dim=-1 for the Softmax operation, we are ensuring that the Softmax is applied across the num_experts dimension, which means that for each data point in the batch, you get a probability distribution over all experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelGatingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_experts):\n",
    "        super(LanguageModelGatingNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Transformer layer\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        #MoE with gating network\n",
    "        self.moe = MixtureOfExpertsGatingNet(num_experts, hidden_size, hidden_size, output_size)\n",
    "    \n",
    "        #fully connected network\n",
    "        self.fc = nn.Linear(output_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output = self.transformer_encoder(embedded)\n",
    "        output = self.moe(output)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our small language model with MoEs and gating network is complete now. Please note that this architecture is oversimplified. In practice, we should add activation functions, perform normalization, and probably use dropout. \n",
    "Non-linear activation functions introduce non-linearities into the model, which helps the network to model complex patterns. Batch normalization or layer normalization can help stabilize the learning process and reduce the training time. In the context of transformer models, layer normalization is typically more common. Dropout is a regularization technique used to prevent overfitting. It randomly sets a fraction of input units to 0 at each update during training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the above language model to incorporate these concepts. I also added a few more layers to make the model deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelGatingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_experts, num_heads, num_layers, dropout_prob=0.1):\n",
    "        super(LanguageModelGatingNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Transformer layer with dropout and normalization\n",
    "        encoder_norm = nn.LayerNorm(embedding_dim)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, hidden_size, dropout=dropout_prob, activation=\"gelu\", norm1=encoder_norm, norm2=encoder_norm)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # MoE with gating network\n",
    "        self.moe = MixtureOfExpertsGatingNet(num_experts, hidden_size, hidden_size, output_size)\n",
    "    \n",
    "        # Enhanced Fully connected network with additional layer and activation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(output_size, output_size),  # Additional layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Linear(output_size, vocab_size)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output = self.transformer_encoder(embedded)\n",
    "        output = self.moe(output)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Switch Trandformers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing from the principles of MoEs and gating networks, William Fedus et al. introduced the Switch Transformers in <a href='https://arxiv.org/abs/2101.03961'>this paper</a>. Switch Transformers incorporate experts within their design. By integrating experts, we can amplify the number of parameters, which can subsequently enhance a model's performance, provided that there's an ample dataset. Additionally, as touched upon earlier, each expert, by design, specializes in a unique task. This allows the model to concentrate on specific segments or combinations of the input. In the realm of language models, this pertains to processing texts or, more precisely, sequences of tokens. However, a challenge with augmenting parameters is the consequent rise in computational demands. Yet, in their paper titled \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\", William Fedus et al. contend that by selecting only the top-scoring expert, it's feasible to harness the merits of MoEs without a drastic surge in floating point operations (FLOPs). Switch Transformers, a variant of the transformer architecture, use a gating network mechanism to 'switch' between multiple 'experts' or modules based on the input, hence their name. Instead of aggregating the outputs from all these experts and passing the combined result forward, only the expert with the highest score is selected for routing the computation. Consequently, even though the addition of a network of experts increases the number of parameters, the total number of operations doesn't see a proportional rise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implentation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchLayer(nn.Module):\n",
    "    def __init__(self, dim, num_experts, hidden_size):\n",
    "        super(SwitchLayer, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([nn.Linear(dim, hidden_size) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Gating mechanism\n",
    "        gates = nn.functional.softmax(self.gate(x), dim=-1)\n",
    "        max_gate_indices = gates.argmax(dim=-1)\n",
    "\n",
    "        # Initialize an empty tensor to store the outputs\n",
    "        outputs = torch.zeros(x.shape[0], x.shape[1], self.experts[0].out_features).to(x.device)  # Ensure proper size and device\n",
    "\n",
    "        # Iterate over each expert\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for tokens that should be routed to this expert\n",
    "            mask = (max_gate_indices == i).float().unsqueeze(-1).expand_as(x)  # Ensure mask shape matches x's shape\n",
    "            # Apply the expert on x and store the result only for tokens routed to this expert\n",
    "            outputs += mask * expert(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModelSwitchNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_experts, num_heads, num_layers, dropout_prob=0.1):\n",
    "        super(LanguageModelSwitchNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Transformer layer with dropout and normalization\n",
    "        encoder_norm = nn.LayerNorm(embedding_dim)\n",
    "#         encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, hidden_size, dropout=dropout_prob, activation=\"gelu\", norm1=encoder_norm, norm2=encoder_norm)\n",
    "#         self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_heads, hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Switch layer instead of MoE\n",
    "        self.switch = SwitchLayer(embedding_dim, num_experts, hidden_size)\n",
    "    \n",
    "        # Fully connected network with additional layer and activation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_size, output_size),  # Additional layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Linear(output_size, vocab_size)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output = self.transformer_encoder(embedded)\n",
    "        output = self.switch(output)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the foundational step in constructing an NLP model involves preprocessing and tokenizing the text data. For this purpose, I employ the SentencePiece tokenizer, which I previously discussed in <a href=https://github.com/lsafarne/AIBites/blob/main/text_tokenization.ipynb>my post about tokenization</a>. There, I detailed the mechanics behind the SentencePiece tokenizer and the process of creating one. I am utilizing the same dataset as in my previous post, the Penn Treebank (PTB), which is readily available through PyTorch's built-in datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from torchtext.datasets import PennTreebank\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('models/SentencePiecePennTree.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PTBDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=30):\n",
    "        tokens = sp.encode_as_ids(data)\n",
    "        self.data = [tokens[i:i+seq_len] for i in range(len(tokens) - seq_len)]\n",
    "        self.targets = [tokens[i+1:i+seq_len+1] for i in range(len(tokens) - seq_len)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "def get_raw_text_from_dataset(dataset):\n",
    "    res = ''\n",
    "    for sentence in dataset:\n",
    "        res = res + sentence\n",
    "    return res\n",
    "\n",
    "train_raw_text = get_raw_text_from_dataset(PennTreebank(split='train'))\n",
    "valid_raw_text = get_raw_text_from_dataset(PennTreebank(split='valid'))\n",
    "\n",
    "train_dataset = PTBDataset(train_raw_text)\n",
    "valid_dataset = PTBDataset(valid_raw_text)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch: 1 | train_Loss: 8.981873512268066\n",
      "Epoch: 1 | Batch: 501 | train_Loss: 6.218678951263428\n",
      "Epoch: 1 | Batch: 1001 | train_Loss: 6.171170711517334\n",
      "Epoch: 1 | Batch: 1501 | train_Loss: 6.016025066375732\n",
      "Epoch: 1 | Batch: 2001 | train_Loss: 6.175940990447998\n",
      "Epoch: 1 | Batch: 2501 | train_Loss: 6.164477348327637\n",
      "Epoch: 1 | Batch: 3001 | train_Loss: 6.1830854415893555\n",
      "Epoch: 1 | Batch: 3501 | train_Loss: 6.147299289703369\n",
      "Epoch: 1 | Batch: 4001 | train_Loss: 6.000978469848633\n",
      "Epoch: 1 | Batch: 4501 | train_Loss: 6.111071586608887\n",
      "Epoch: 1 | Batch: 5001 | train_Loss: 6.032265663146973\n",
      "Epoch: 1 | Batch: 5501 | train_Loss: 6.150510787963867\n",
      "Epoch: 1 | Batch: 6001 | train_Loss: 5.826602458953857\n",
      "Epoch: 1 | Batch: 6501 | train_Loss: 6.220767021179199\n",
      "Epoch: 1 | Batch: 7001 | train_Loss: 6.211997032165527\n",
      "Epoch: 1 | Batch: 7501 | train_Loss: 6.179198741912842\n",
      "Epoch: 1 | Batch: 8001 | train_Loss: 6.285243034362793\n",
      "Epoch: 1 | Batch: 8501 | train_Loss: 6.031949520111084\n",
      "Epoch: 1 | Batch: 9001 | train_Loss: 6.066867351531982\n",
      "Epoch: 1 | Batch: 9501 | train_Loss: 6.226119518280029\n",
      "Epoch: 1 | Batch: 10001 | train_Loss: 6.220879077911377\n",
      "Epoch: 1 | Batch: 10501 | train_Loss: 6.092916488647461\n",
      "Epoch: 1 | Batch: 11001 | train_Loss: 6.016939640045166\n",
      "Epoch: 1 | Batch: 11501 | train_Loss: 6.019294738769531\n",
      "Epoch: 1 | Batch: 12001 | train_Loss: 6.165482044219971\n",
      "Epoch: 1 | Batch: 12501 | train_Loss: 6.190398216247559\n",
      "Epoch: 1 | Batch: 13001 | train_Loss: 5.9487786293029785\n",
      "Epoch: 1 | Batch: 13501 | train_Loss: 6.131190776824951\n",
      "Epoch: 1 | Batch: 14001 | train_Loss: 6.041342735290527\n",
      "Epoch: 1 | Batch: 14501 | train_Loss: 5.922934532165527\n",
      "Epoch: 1 | Batch: 15001 | train_Loss: 6.191397190093994\n",
      "Epoch: 1 | Batch: 15501 | train_Loss: 6.30873441696167\n",
      "Epoch: 1 | Batch: 16001 | train_Loss: 6.059210777282715\n",
      "Epoch: 1 | Batch: 16501 | train_Loss: 6.186244010925293\n",
      "Epoch: 1 | Batch: 17001 | train_Loss: 6.38538932800293\n",
      "Epoch: 1 | Batch: 17501 | train_Loss: 5.895572185516357\n",
      "Epoch: 1 | Batch: 18001 | train_Loss: 6.252148151397705\n",
      "Epoch: 1 | Batch: 18501 | train_Loss: 6.173130989074707\n",
      "Epoch: 1 | Batch: 19001 | train_Loss: 5.94216251373291\n",
      "Epoch: 1 | Batch: 19501 | train_Loss: 6.0411505699157715\n",
      "Epoch: 1 | Batch: 1 | validation_Loss: 5.432121753692627\n",
      "Epoch: 1 | Batch: 101 | validation_Loss: 4.473713397979736\n",
      "Epoch: 1 | Batch: 201 | validation_Loss: 6.357522964477539\n",
      "Epoch: 1 | Batch: 301 | validation_Loss: 6.7406182289123535\n",
      "Epoch: 1 | Batch: 401 | validation_Loss: 6.25815486907959\n",
      "Epoch: 1 | Batch: 501 | validation_Loss: 4.9168806076049805\n",
      "Epoch: 1 | Batch: 601 | validation_Loss: 6.205606460571289\n",
      "Epoch: 1 | Batch: 701 | validation_Loss: 5.747393608093262\n",
      "Epoch: 1 | Batch: 801 | validation_Loss: 6.3318634033203125\n",
      "Epoch: 1 | Batch: 901 | validation_Loss: 6.069680213928223\n",
      "Epoch: 1 | Batch: 1001 | validation_Loss: 6.16057825088501\n",
      "Epoch: 1 | Batch: 1101 | validation_Loss: 6.205451488494873\n",
      "Epoch: 1 | Batch: 1201 | validation_Loss: 6.3403425216674805\n",
      "Epoch: 1 | Batch: 1301 | validation_Loss: 6.744630336761475\n",
      "Epoch: 1 | Batch: 1401 | validation_Loss: 5.44295597076416\n",
      "Epoch: 1 | Batch: 1501 | validation_Loss: 6.7303147315979\n",
      "After epoch 1: Average Train Loss: 6.1298, Average Validation Loss: 6.1120\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = sp.get_piece_size()\n",
    "embedding_dim = 256 # Can adjust based on our needs\n",
    "hidden_size=256 \n",
    "output_size=512 \n",
    "num_heads=8 \n",
    "num_layers=6\n",
    "num_experts=32\n",
    "\n",
    "# Initialize the model\n",
    "model = LanguageModelSwitchNet(vocab_size, embedding_dim, hidden_size, output_size, num_experts, num_heads, num_layers, dropout_prob=0.1)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Lists for storing losses for each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print(f\"Epoch: {epoch+1} | Batch: {batch_idx+1} | train_Loss: {loss.item()}\")\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "    # Calculate the average training loss for this epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(valid_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            valid_loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            val_loss += valid_loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1} | Batch: {batch_idx+1} | validation_Loss: {valid_loss.item()}\")\n",
    "                val_losses.append(valid_loss)\n",
    "        # Calculate the average validation loss for this epoch\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "    print(f\"After epoch {epoch+1}: Average Train Loss: {avg_train_loss:.4f}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Mixture of Experts</b>: In a MoE model, there are multiple expert networks, and each one is responsible for handling a specific subset or type of data. Experts are individual models or subnetworks, each trained to specialize in a different aspect of the data.</li>\n",
    "    <li><b>Gating Network</b> For a given input, the gating network outputs a weight for each expert. The final output is a weighted sum of the experts' outputs based on the gating network's weights. These weights determine how much each expert contributes to the final output.</li>\n",
    "    <li><b>Switch Transformers</b>: These transformers use a gating mechanism to direct input to only one 'expert' module, the one with the highest score, rather than computing and forwarding the weighted sum of the experts' outputs, thereby optimizing computational efficiency. This method expands model capacity without a corresponding increase in floating point operations (FLOPs).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "<h2>What's next</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "In this post and the previous one, I explained what a language model is, how to build a toy language model, and how to train it. In practical scenarios, building a language model, especially a large language model, demands a tremendous amount of data and computational resources. Thus, many companies build on the shoulders of giants by grabbing a language model and fine-tuning it to specialize it for their specific task at hand. In the next post, I will explain how to fine-tune a large language model step by step with detailed explanations and implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seqvenv",
   "language": "python",
   "name": "seq2seqvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}