{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Finetuning a Large Language Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Overview</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will delve into the process of finetuning a pretrained language model with the aid of the HuggingFace library. Previously, we explored the fundamentals of language models and the methodology behind constructing one. Nonetheless, the development of large and sophisticated language models requires immense computational resources and extensive datasets, assets not readily available to everyone. Consequently, an efficient alternative involves leveraging existing large language models (LLMs) instead of creating one from scratch.\n",
    "\n",
    "While this strategy significantly conserves computational resources, there's a caveat: pretrained models, standardized for general tasks, may not perform optimally on our distinct dataset for specific tasks. This is primarily because they may not have encountered samples during training that resonate with the unique characteristics of our dataset. Therefore, to tailor a pretrained language model to our specific needs while still capitalizing on the rich knowledge encapsulated within its parameters, we can employ a strategy known as finetuning.\n",
    "\n",
    "Finetuning allows us to specialize a model's abilities: we take an established model and continue its training regimen, but this time, using our dataset. This method does not only instill an understanding of our dataset's particularities in the model but also builds upon the expansive learning the model has already achieved. In essence, we're not initiating the learning process from scratch, but rather, standing on the shoulders of giants—benefiting from the pretrained model's extensive learning and adapting it to our specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, it's crucial to acknowledge that finetuning, despite its advantages, isn't without its challenges. A primary concern is the risk of overfitting. Overfitting occurs when a model learns the training data too well, to the point where it captures noise and inaccuracies as patterns. This typically happens if the training data is too limited or the model is excessively complex relative to the task. As a result, while the model might perform exceptionally well on the training data, its ability to generalize to new, unseen data is compromised.\n",
    "\n",
    "In the context of finetuning a language model, overfitting might manifest if the model is trained too long or too intensely on a small, specific dataset. While the pretrained model has learned from a vast amount of data, it risks becoming too specialized in the narrow task or dataset it's finetuned on, thereby losing its ability to perform well on tasks outside this specific context.\n",
    "\n",
    "There are strategies to mitigate overfitting, such as early stopping, which involves ending training when performance on a validation dataset starts to deteriorate, or employing regularization techniques. However, it remains a significant consideration when deciding the extent to which finetuning should be carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aim to keep this content concise and focused. Therefore, I'll explore alternatives to finetuning in a subsequent post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example task that I will demonstrate to show how to fine-tune a pretrained model is text classification for sentiment analysis.\n",
    "\n",
    "As previously stated, I utilize a model and dataset from Hugging Face for fine-tuning purposes. Hugging Face is an AI platform renowned for its robust infrastructure, which enables users to share, access, and showcase their models. The platform offers a diverse array of models, as well as a comprehensive collection of datasets. It is particularly celebrated for its 'Transformers' library, a tool that simplifies the incorporation of the Transformer architecture in model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I will use is the Stanford Sentiment Treebank 2 (SST-2). This dataset is popular in natural language processing, particularly for tasks related to sentiment analysis. It's an extension of the original Stanford Sentiment Treebank (SST) and is widely used for benchmarking models in the field of sentiment analysis. You can find more information on this dataset <a href=https://huggingface.co/datasets/sst2>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset and look at a few examples below. Remember to install the 'transformers' and 'datasets' libraries if you haven't already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#%pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: hide new secretions from the parental units \n",
      "Label: Negative\n",
      "\n",
      "Sentence: contains no wit , only labored gags \n",
      "Label: Negative\n",
      "\n",
      "Sentence: that loves its characters and communicates something rather beautiful about human nature \n",
      "Label: Positive\n",
      "\n",
      "Sentence: remains utterly satisfied to remain the same throughout \n",
      "Label: Negative\n",
      "\n",
      "Sentence: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
      "Label: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Sentence:\", dataset[\"train\"][i][\"sentence\"])\n",
    "    print(\"Label:\", \"Positive\" if dataset[\"train\"][i][\"label\"] == 1 else \"Negative\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset</h3>\n",
    "Remember, data preprocessing is always the foundational step in building any deep learning or machine learning model. This is especially true for NLP tasks, where one crucial initial step is tokenization. In our case, since we're using a publicly available dataset from Hugging Face, there isn't much required in terms of dataset preparation beyond simply loading it. As for the tokenization process, you can refer to my detailed post about it <a href=https://github.com/lsafarne/AIBites/blob/main/text_tokenization.ipynb>here</a>. However, for this project, we will be utilizing an AutoTokenizer from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model</h3>\n",
    "The model that I will use is DistilBERT, which is a streamlined version of BERT. BERT (Bidirectional Encoder Representations from Transformers), developed by Google, is a transformer-based model known for learning contextual word embeddings. It utilizes a self-supervised learning paradigm, training on large volumes of text by masking words and predicting them, thereby gaining a deep contextual understanding of language. DistilBERT, developed by Hugging Face, is essentially a distilled version of BERT. It is 40% smaller but retains 97% of BERT's language understanding capabilities and can be trained much faster. This makes DistilBERT an attractive choice for applications where model size and speed are crucial, without significantly compromising the quality of the results. I chose DistilBERT since I am running this notebook on my laptop and do not have access to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer , DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer):\n",
    "    model.train() # Put the model into training mode\n",
    "    total_train_loss = 0\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad() # Zero the gradients at the start of the iteration\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device).long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the average loss over all the batches\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    print(\"Average training loss: \", avg_train_loss)\n",
    "\n",
    "    return avg_train_loss\n",
    "\n",
    "# Load a dataset and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "raw_datasets = load_dataset(\"sst\", \"default\")\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "\n",
    "# Format the dataset to output torch.Tensor\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Load pre-trained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare for training\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_dataloader, optimizer)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned earlier, I will use Hugging Face's transformers library. In particular, I will use the <b>AutoModelForSequenceClassification</b> and <b>AutoTokenizer</b> classes, which are part of the <b>AutoClasses</b> in the transformers library. AutoClasses are designed to make it easier to work with multiple models without having to significantly change your code. They handle the underlying details and provide a more seamless experience when working with different model architectures. You can find more information on these classes <a href=https://huggingface.co/docs/transformers/model_doc/auto>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Step 2: Load the tokenizer and model\n",
    "model_checkpoint = \"distilbert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2) # SST-2 is binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of lines 7, 8, and 9 in the code snippet above. These lines demonstrate how the AutoClasses from the Transformers library can simplify the coding process. Additionally, if you wish to use a different model, you can easily do so by simply changing the model name to any other model available on the Hugging Face hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./sentiment_model\")\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seqvenv",
   "language": "python",
   "name": "seq2seqvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
