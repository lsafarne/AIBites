{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Finetuning a Large Language Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Overview</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will delve into the process of finetuning a pretrained language model with the aid of the HuggingFace library. Previously, we explored the fundamentals of language models and the methodology behind constructing one. Nonetheless, the development of large and sophisticated language models requires immense computational resources and extensive datasets, assets not readily available to everyone. Consequently, an efficient alternative involves leveraging existing large language models (LLMs) instead of creating one from scratch.\n",
    "\n",
    "While this strategy significantly conserves computational resources, there's a caveat: pretrained models, standardized for general tasks, may not perform optimally on our distinct dataset for specific tasks. This is primarily because they may not have encountered samples during training that resonate with the unique characteristics of our dataset. Therefore, to tailor a pretrained language model to our specific needs while still capitalizing on the rich knowledge encapsulated within its parameters, we can employ a strategy known as finetuning.\n",
    "\n",
    "Finetuning allows us to specialize a model's abilities: we take an established model and continue its training regimen, but this time, using our dataset. This method does not only instill an understanding of our dataset's particularities in the model but also builds upon the expansive learning the model has already achieved. In essence, we're not initiating the learning process from scratch, but rather, standing on the shoulders of giants—benefiting from the pretrained model's extensive learning and adapting it to our specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, it's crucial to acknowledge that finetuning, despite its advantages, isn't without its challenges. A primary concern is the risk of overfitting. Overfitting occurs when a model learns the training data too well, to the point where it captures noise and inaccuracies as patterns. This typically happens if the training data is too limited or the model is excessively complex relative to the task. As a result, while the model might perform exceptionally well on the training data, its ability to generalize to new, unseen data is compromised.\n",
    "\n",
    "In the context of finetuning a language model, overfitting might manifest if the model is trained too long or too intensely on a small, specific dataset. While the pretrained model has learned from a vast amount of data, it risks becoming too specialized in the narrow task or dataset it's finetuned on, thereby losing its ability to perform well on tasks outside this specific context.\n",
    "\n",
    "There are strategies to mitigate overfitting, such as early stopping, which involves ending training when performance on a validation dataset starts to deteriorate, or employing regularization techniques. However, it remains a significant consideration when deciding the extent to which finetuning should be carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aim to keep this content concise and focused. Therefore, I'll explore alternatives to finetuning in a subsequent post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example task that I will demonstrate to show how to fine-tune a pretrained model is text classification for sentiment analysis.\n",
    "\n",
    "As previously stated, I utilize a model and dataset from Hugging Face for fine-tuning purposes. Hugging Face is an AI platform renowned for its robust infrastructure, which enables users to share, access, and showcase their models. The platform offers a diverse array of models, as well as a comprehensive collection of datasets. It is particularly celebrated for its 'Transformers' library, a tool that simplifies the incorporation of the Transformer architecture in model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I will use is the Stanford Sentiment Treebank 2 (SST-2). This dataset is popular in natural language processing, particularly for tasks related to sentiment analysis. It's an extension of the original Stanford Sentiment Treebank (SST) and is widely used for benchmarking models in the field of sentiment analysis. You can find more information on this dataset <a href=https://huggingface.co/datasets/sst2>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset and look at a few examples below. Remember to install the 'transformers' and 'datasets' libraries if you haven't already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#%pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: hide new secretions from the parental units \n",
      "Label: Negative\n",
      "\n",
      "Sentence: contains no wit , only labored gags \n",
      "Label: Negative\n",
      "\n",
      "Sentence: that loves its characters and communicates something rather beautiful about human nature \n",
      "Label: Positive\n",
      "\n",
      "Sentence: remains utterly satisfied to remain the same throughout \n",
      "Label: Negative\n",
      "\n",
      "Sentence: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
      "Label: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Sentence:\", dataset[\"train\"][i][\"sentence\"])\n",
    "    print(\"Label:\", \"Positive\" if dataset[\"train\"][i][\"label\"] == 1 else \"Negative\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset</h3>\n",
    "Remember, data preprocessing is always the foundational step in building any deep learning or machine learning model. This is especially true for NLP tasks, where one crucial initial step is tokenization. In our case, since we're using a publicly available dataset from Hugging Face, there isn't much required in terms of dataset preparation beyond simply loading it. As for the tokenization process, you can refer to my detailed post about it <a href=https://github.com/lsafarne/AIBites/blob/main/text_tokenization.ipynb>here</a>. However, for this project, we will be utilizing an AutoTokenizer from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer \n",
    "\n",
    "# Load a dataset and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "raw_datasets = load_dataset(\"sst\", \"default\")\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "\n",
    "# Format the dataset to output torch.Tensor\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model</h3>\n",
    "The model that I will use is DistilBERT, which is a streamlined version of BERT. BERT (Bidirectional Encoder Representations from Transformers), developed by Google, is a transformer-based model known for learning contextual word embeddings. It utilizes a self-supervised learning paradigm, training on large volumes of text by masking words and predicting them, thereby gaining a deep contextual understanding of language. DistilBERT, developed by Hugging Face, is essentially a distilled version of BERT. It is 40% smaller but retains 97% of BERT's language understanding capabilities and can be trained much faster. This makes DistilBERT an attractive choice for applications where model size and speed are crucial, without significantly compromising the quality of the results. I chose DistilBERT since I am running this notebook on my laptop and do not have access to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Load pre-trained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer , DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, num_epochs):\n",
    "    model.train() # Put the model into training mode\n",
    "    total_train_loss = 0\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Iterate over the batches\n",
    "        for batch in tqdm(dataloader, desc= f\"Training ({epoch+1}/{num_epochs})\"):\n",
    "            optimizer.zero_grad() # Zero the gradients at the start of the iteration\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device).long()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate the average loss over all the batches\n",
    "        avg_train_loss = total_train_loss / len(dataloader)\n",
    "        print(f\"Average training loss for epoch {epoch+1}: \", avg_train_loss)\n",
    "\n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "\n",
    "# Prepare for training\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "# Train the model\n",
    "train(model, train_dataloader, optimizer, num_epochs)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    total_eval_loss = 0\n",
    "    total_eval_accuracy = 0\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Extract data and labels\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device).long()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss and accuracy\n",
    "            loss = criterion(logits, labels)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_eval_loss += loss.item()\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        \n",
    "\n",
    "    average_loss = total_eval_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total\n",
    "\n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "# Prepare the test dataloader\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_loss, eval_accuracy = evaluate(model, test_dataloader)\n",
    "\n",
    "# Print final evaluation results\n",
    "print(f\"Final evaluation loss: {eval_loss}\")\n",
    "print(f\"Final evaluation accuracy: {eval_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to measuring the accuracy, let's perform inference for a few samples that we saw earlier in this post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: uneasy mishmash of styles and genres .\n",
      "True Label: Negative\n",
      "Predicted Label: Negative\n",
      "\n",
      "Sentence: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .\n",
      "True Label: Negative\n",
      "Predicted Label: Negative\n",
      "\n",
      "Sentence: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
      "True Label: Negative\n",
      "Predicted Label: Negative\n",
      "\n",
      "Sentence: director rob marshall went out gunning to make a great one .\n",
      "True Label: Negative\n",
      "Predicted Label: Negative\n",
      "\n",
      "Sentence: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .\n",
      "True Label: Negative\n",
      "Predicted Label: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "for i in range(5):\n",
    "    # Extract the sentence and its true label\n",
    "    sentence = dataset[\"test\"][i][\"sentence\"]\n",
    "    true_label = \"Positive\" if dataset[\"test\"][i][\"label\"] == 1 else \"Negative\"\n",
    "\n",
    "    # Tokenize the sentence - Note that we should use the same tokenizer as the one we used to preprocess our dataset\n",
    "    inputs = tokenizer.encode_plus(sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Extract the prediction\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    predicted_label = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "    # Print the sentence, the true label, and the predicted label\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"True Label:\", true_label)\n",
    "    print(\"Predicted Label:\", predicted_label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finetuned model has predicted all labels for this set of examples correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hugging Face AutoClasses</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly utilizing the DistilBertForSequenceClassification model, we have the option to employ the AutoModelForSequenceClassification and AutoTokenizer classes. These are components of the AutoClasses within the transformers library. The design of AutoClasses facilitates ease in working with various models, reducing the need for substantial code alterations. They effectively manage underlying complexities, offering a smoother experience across different model architectures. Further details on these classes can be accessed <a href=https://huggingface.co/docs/transformers/model_doc/auto>here</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\" \n",
    "auto_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1) # SST-2 is binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two lines demonstrate how the AutoClasses from the Transformers library can simplify the coding process. Additionally, if you wish to use a different model, you can easily do so by simply changing the model name to any other model available on the Hugging Face hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, instead of utilizing the aforementioned training loop, the Trainer class could be employed. This approach offers the advantage of streamlining and condensing the code. Nonetheless, I have a stronger familiarity with the original training loop; its structure offers greater ease in debugging if any issues arise. To provide a comprehensive overview, I will include an example of how to implement the Trainer class below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "auto_model.to(device)\n",
    "\n",
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=auto_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./sentiment_model\")\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#%pip install accelerate -U\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seqvenv",
   "language": "python",
   "name": "seq2seqvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
