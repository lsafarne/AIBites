{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Finetuning a Large Language Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Overview</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will delve into the process of finetuning a pretrained language model with the aid of the HuggingFace library. Previously, we explored the fundamentals of language models and the methodology behind constructing one. Nonetheless, the development of large and sophisticated language models requires immense computational resources and extensive datasets, assets not readily available to everyone. Consequently, an efficient alternative involves leveraging existing large language models (LLMs) instead of creating one from scratch.\n",
    "\n",
    "While this strategy significantly conserves computational resources, there's a caveat: pretrained models, standardized for general tasks, may not perform optimally on our distinct dataset for specific tasks. This is primarily because they may not have encountered samples during training that resonate with the unique characteristics of our dataset. Therefore, to tailor a pretrained language model to our specific needs while still capitalizing on the rich knowledge encapsulated within its parameters, we can employ a strategy known as finetuning.\n",
    "\n",
    "Finetuning allows us to specialize a model's abilities: we take an established model and continue its training regimen, but this time, using our dataset. This method does not only instill an understanding of our dataset's particularities in the model but also builds upon the expansive learning the model has already achieved. In essence, we're not initiating the learning process from scratch, but rather, standing on the shoulders of giants—benefiting from the pretrained model's extensive learning and adapting it to our specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, it's crucial to acknowledge that finetuning, despite its advantages, isn't without its challenges. A primary concern is the risk of overfitting. Overfitting occurs when a model learns the training data too well, to the point where it captures noise and inaccuracies as patterns. This typically happens if the training data is too limited or the model is excessively complex relative to the task. As a result, while the model might perform exceptionally well on the training data, its ability to generalize to new, unseen data is compromised.\n",
    "\n",
    "In the context of finetuning a language model, overfitting might manifest if the model is trained too long or too intensely on a small, specific dataset. While the pretrained model has learned from a vast amount of data, it risks becoming too specialized in the narrow task or dataset it's finetuned on, thereby losing its ability to perform well on tasks outside this specific context.\n",
    "\n",
    "There are strategies to mitigate overfitting, such as early stopping, which involves ending training when performance on a validation dataset starts to deteriorate, or employing regularization techniques. However, it remains a significant consideration when deciding the extent to which finetuning should be carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aim to keep this content concise and focused. Therefore, I'll explore alternatives to finetuning in a subsequent post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example task that I will demonstrate to show how to fine-tune a pretrained model is text classification for sentiment analysis.\n",
    "\n",
    "As previously stated, I utilize a model and dataset from Hugging Face for fine-tuning purposes. Hugging Face is an AI platform renowned for its robust infrastructure, which enables users to share, access, and showcase their models. The platform offers a diverse array of models, as well as a comprehensive collection of datasets. It is particularly celebrated for its 'Transformers' library, a tool that simplifies the incorporation of the Transformer architecture in model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I will use is the Stanford Sentiment Treebank 2 (SST-2). This dataset is popular in natural language processing, particularly for tasks related to sentiment analysis. It's an extension of the original Stanford Sentiment Treebank (SST) and is widely used for benchmarking models in the field of sentiment analysis. You can find more information on this dataset <a href=https://huggingface.co/datasets/sst2>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset and look at a few examples below. Remember to install the 'transformers' and 'datasets' libraries if you haven't already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: hide new secretions from the parental units \n",
      "Label: Negative\n",
      "\n",
      "Sentence: contains no wit , only labored gags \n",
      "Label: Negative\n",
      "\n",
      "Sentence: that loves its characters and communicates something rather beautiful about human nature \n",
      "Label: Positive\n",
      "\n",
      "Sentence: remains utterly satisfied to remain the same throughout \n",
      "Label: Negative\n",
      "\n",
      "Sentence: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
      "Label: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Sentence:\", dataset[\"train\"][i][\"sentence\"])\n",
    "    print(\"Label:\", \"Positive\" if dataset[\"train\"][i][\"label\"] == 1 else \"Negative\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that I will use is DistilBERT, which is a streamlined version of BERT. BERT (Bidirectional Encoder Representations from Transformers), developed by Google, is a transformer-based model known for learning contextual word embeddings. It utilizes a self-supervised learning paradigm, training on large volumes of text by masking words and predicting them, thereby gaining a deep contextual understanding of language. DistilBERT, developed by Hugging Face, is essentially a distilled version of BERT. It is 40% smaller but retains 97% of BERT's language understanding capabilities and can be trained much faster. This makes DistilBERT an attractive choice for applications where model size and speed are crucial, without significantly compromising the quality of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned earlier, I will use Hugging Face's transformers library. In particular, I will use the <b>AutoModelForSequenceClassification</b> and <b>AutoTokenizer</b> classes, which are part of the <b>AutoClasses</b> in the transformers library. AutoClasses are designed to make it easier to work with multiple models without having to significantly change your code. They handle the underlying details and provide a more seamless experience when working with different model architectures. You can find more information on these classes <a href=https://huggingface.co/docs/transformers/model_doc/auto>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seqvenv",
   "language": "python",
   "name": "seq2seqvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
