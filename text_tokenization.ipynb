{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenization in Modern Language Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Overview</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is designed to introduce two prominent tokenization methods, Byte Pair Encoding (BPE) and SentencePiece, which are commonly employed in State-of-the-Art (SOTA) Large Language Models such as the GPT series, ALBERT, and T5.\n",
    "\n",
    "Initially, I will provide an overview of traditional tokenization techniques, outlining their limitations, and explaining how the more recent methods of BPE and SentencePiece overcome these challenges. Next, I will delve into the underlying concepts behind BPE and SentencePiece, giving you a detailed understanding of how they work. Finally, I'll offer a simple example to demonstrate how these methods can be implemented in Python. To be clear, we won't be coding them from scratch but rather utilizing existing libraries to take advantage of these powerful tokenization techniques.\n",
    "\n",
    "For those interested in exploring further, the SentencePiece paper can be found <a href='https://arxiv.org/abs/1808.06226'>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenization in Natural Language Processing: A General Overview</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tokenization is the process of breaking text down into a sequence of tokens. For example, the sentence \"The sky is blue\" is broken down into ['The', 'sky', 'is', 'blue']. A tokenizer is the tool that implements this tokenization process. Please note that the commonly used term is 'token' rather than 'word,' since tokens may not always correspond to clean English (or any other language) words, such as 'sky' or 'blue'. Instead, you might encounter tokens like '##ve' or 'lo', which may seem meaningless but are quite useful for language models and other NLP applications.\n",
    "\n",
    "The tokenization process often overlaps with preprocessing tasks like converting all tokens to lowercase, lemmatization or stemming, and removing stop words, among others. As a result of tokenization, one can create a vocabulary, which is the set of all possible tokens in a text corpus. In addition, we will have a tool—the tokenizer—that can break any given text into a sequence of tokens from the vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One trivial way of tokenization is simply splitting a text into its constituent words using the spaces between the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sky', 'is', 'blue']\n"
     ]
    }
   ],
   "source": [
    "text = 'The sky is blue'\n",
    "tokenized_text = text.split(' ')\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, splitting a text using merely a space is not an optimum way since a text usually includes punctuation, and words might have different forms, etc. For example, given the sentence 'The fact is: the sky has not been blue in the past three days,' should we treat 'is' and 'been' as the same token? Should we convert all letters to lower (upper) case? Should we remove the ':' after the word 'is,' or keep it? These are some natural questions and scenarios that we should answer before tokenizing our texts. Additionally, do not forget that after designing our tokenizer, we should go over all the texts that we have in our corpus and tokenize them to build our vocabulary, which is the set of all tokens in our corpus.\n",
    "\n",
    "There are a few libraries that can help us perform better tokenization rather than simply splitting the text. In the following, I will give two examples: one using spaCy, and the other using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lidasafarnejad/PycharmProjects/seq2seqModeling/seq2seqvenv/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.6.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'The', 'sky', 'is', 'blue', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Hello, world! The sky is blue.\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ignore the warning. It simply explains that I am using spaCy version 3.6.1, while the model 'en_core_web_sm' was trained using spaCy version 3.0.0.\n",
    "\n",
    "The model 'en_core_web_sm' is a versatile pre-trained model that can perform various NLP tasks beyond tokenization. Some of these tasks include Part-of-Speech Tagging, Named Entity Recognition, and Dependency Parsing, to name just three.\"\n",
    "\n",
    "Here is another example of tokenization using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'The', 'sun', 'is', 'shinning', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello, world! The sun is shinning.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to download the Punkt tokenizer models using the following line of code:\n",
    "<i><b>nltk.download('punkt')</b></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How about out-of-vocabulary tokens?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One immediate question that may arise is how to handle a sentence containing tokens not found in the vocabulary. How should we tokenize it? A common heuristic solution is to replace unseen or unknown tokens with a pre-defined symbol like <UNKNOWN>. However, this approach might overlook tokens that carry significant information, potentially affecting the performance of NLP models. Additionally, dealing with punctuation and special characters often relies on heuristic methods and may vary according to the preferences or standards set by individual developers.\n",
    "\n",
    "More recently proposed tokenization methods aim to address these limitations by handling out-of-vocabulary words and special tokens or characters in a more standardized and consistent manner.\n",
    "\n",
    "In what follows, I will delve into BPE and SentencePiece, the two most recent tokenization techniques that are widely employed with state-of-the-art language models.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Byte Pair Encoding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE, or Byte-Pair Encoding, is a <i><b>subword</b></i> tokenization method, meaning it breaks down words into smaller units known as subwords. Unlike traditional tokenization, which usually divides texts into individual words, BPE goes a step further by segmenting words into even smaller parts. For instance, the word 'love' might be broken down into 'lo' and 've', and the word 'unhappy' might be segmented into 'un' and 'happy'.\n",
    "\n",
    "BPE is a data-driven algorithm, meaning that it calculates the frequencies of character pairs and sequences based on the specific data on which it is trained. This ensures that the resulting tokenizer is precisely tailored to the dataset at hand, allowing it to capture unique characteristics within that data.\n",
    "\n",
    "Unlike methods that rely on a predefined vocabulary, BPE constructs its own vocabulary based on the particular corpus it is trained on. This approach enables the model to detect and capture special patterns within the texts of that corpus. Furthermore, by segmenting words into subwords, BPE has the ability to manage out-of-vocabulary words more effectively. Even words that are not directly present in the vocabulary can be represented through a combination of known subwords, enhancing the model's flexibility and performance.\n",
    "\n",
    "The BPE algorithm begins by splitting a text into individual characters, treating each one as a separate token. It then trains on the data to iteratively identify and merge the most frequent pairs of characters or sequences. This process continues until a specified number of merges have been achieved, or the desired vocabulary size is met. To gain a clearer understanding, let's examine a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose we are given a small corpus with the following frequencies: \n",
    "<ul>\n",
    "    <li>low: 5</li>\n",
    "    <li>lower: 2</li>\n",
    "    <li>newest: 6 </li>\n",
    "    <li>widest: 3</li>\n",
    "</ul>\n",
    "The BPE algorithm starts by breaking down the words into the characters. Here is the resulting characters and their frequencies:\n",
    "<ul>\n",
    "    <li>'l' 'o' 'w': 5</li>\n",
    "    <li>'l' 'o' 'w' 'e' 'r': 2</li>\n",
    "    <li>'n' 'e' 'w' 'e' 's' 't': 6 </li>\n",
    "    <li>'w' 'i' 'd' 'e' 's' 't': 3</li>\n",
    "</ul>\n",
    "First, the characters 'e' and 's' appear 9 times together. Thus, we will merge them and continue. Then the new corpus will look like as follows:\n",
    "<ul>\n",
    "    <li>'l' 'o' 'w': 5</li>\n",
    "    <li>'l' 'o' 'w' 'e' 'r': 2</li>\n",
    "    <li>'n' 'e' 'w' 'es' 't': 6 </li>\n",
    "    <li>'w' 'i' 'd' 'es' 't': 3</li>\n",
    "</ul>\n",
    "Please note we could also choose to merge 's' and 't' because they also appear 9 times together. In such cases, that is when you have <i>ties</i>, you need to have a strategy to handle them. For example, you can randomly choose between them,  you might consider the frequency of indvividual tokens in the pair and choose those with the highest individual tokens, or you might have a predefined order such as alphabetical order.\n",
    "\n",
    "Second iteration: 'es' and 't' appear 9 times together. Merge them to 'est':\n",
    "<ul>\n",
    "    <li>'l' 'o' 'w': 5</li>\n",
    "    <li>'l' 'o' 'w' 'e' 'r': 2</li>\n",
    "    <li>'n' 'e' 'w' 'est': 6 </li>\n",
    "    <li>'w' 'i' 'd' 'est': 3</li>\n",
    "</ul>\n",
    "\n",
    "Third iteration: 'l' and 'o' appear 7 times together. Merge them to 'lo':\n",
    "<ul>\n",
    "    <li>'lo' 'w': 5</li>\n",
    "    <li>'lo' 'w' 'e' 'r': 2</li>\n",
    "    <li>'n' 'e' 'w' 'est': 6 </li>\n",
    "    <li>'w' 'i' 'd' 'est': 3</li>\n",
    "</ul>\n",
    "Fourth Iteration: \"lo\" and \"w\" appear together 7 times. Merge them into \"low\":\n",
    "<ul>\n",
    "    <li>'low': 5</li>\n",
    "    <li>'low' 'e' 'r': 2</li>\n",
    "    <li>'n' 'e' 'w' 'est': 6 </li>\n",
    "    <li>'w' 'i' 'd' 'est': 3</li>\n",
    "</ul>\n",
    "Fifth Iteration: \"n\" and \"e\" appear together 6 times. Merge them into \"ne\":\n",
    "<ul>\n",
    "    <li>'low': 5</li>\n",
    "    <li>'low' 'e' 'r': 2</li>\n",
    "    <li>'ne' 'w' 'est': 6 </li>\n",
    "    <li>'w' 'i' 'd' 'est': 3</li>\n",
    "</ul>\n",
    "\n",
    "I believe you've grasped the idea. The vocabulary is $V={'l', 'o', 'w', 'e', 'r', 'n', 'i', 'd', 's', 't', 'es', 'est', 'lo', 'low', 'ne'}$. We can continue until we either reach a predefined vocabulary size or a predefined number of itetions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing BPE</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There certain ways to implement BPE. You can implement it from scratch, or you could use libraries, such as <b>transformers</b>, <b>tokenizers</b>. or <b>SentencePiece</b> from Hugging Face. In the following, I will show you a few examples how to implement your own BPE tokenizer and how to use the libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing BPE from scratch</h3>\n",
    "\n",
    "The code snippet below illustrates a simplified implementation of our example. It's worth noting that a real-world application might be more complex. For instance, we may need to include strategies to manage ties or deal with special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low: 5\n",
      "low e r: 2\n",
      "ne w est: 6\n",
      "w i d est: 3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {\n",
    "    'l o w': 5,\n",
    "    'l o w e r': 2,\n",
    "    'n e w e s t': 6,\n",
    "    'w i d e s t': 3\n",
    "}\n",
    "\n",
    "num_merges = 5\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "# Printing the final vocabulary\n",
    "for word, freq in vocab.items():\n",
    "    print(f'{word}: {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing using libraries</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>GPT2Tokenizer from the library transformers from Hugging Face:</b>\n",
    "\n",
    "The following code snippets show how we can use from the <b>'transformers'</b> library. Please note that if do not have these libraries you can use <i>!pip install transformers</i> to install them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded text: [40778, 12, 47, 958, 14711, 7656, 318, 257, 850, 4775, 11241, 1634, 2446, 13]\n",
      "dencoded text: Byte-Pair Encoding is a subword tokenization method.\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 tokenizer, which uses BPE\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode a text string using BPE\n",
    "text = \"Byte-Pair Encoding is a subword tokenization method.\"\n",
    "encoded_text = tokenizer.encode(text)\n",
    "print(f'encoded text: {encoded_text}')\n",
    "# Decode the encoded text to see how it's been tokenized\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(f'dencoded text: {decoded_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet <i><b>GPT2Tokenizer.from_pretrained(\"gpt2\")</i></b>, the identifier 'gpt2' speficies the tokenizer that is designed and configured to work specifically with the GPT-2 language model. It includes the rules and vocabulary necessary to convert text into a numerical format that the GPT-2 model can process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ByteLevelBPETokenizer from the tokenizers by Hugging Face:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/vocab.json', 'models/merges.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Path to the corpus file\n",
    "corpus_file = \"data/PennTreeBank.txt\"\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Train the tokenizer on the corpus\n",
    "tokenizer.train(files=[corpus_file], vocab_size=30_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_model(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print a few lines of 'vocab.json' and 'merges.txt' to see what their contents are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "<s> 0\n",
      "<pad> 1\n",
      "</s> 2\n",
      "<unk> 3\n",
      "<mask> 4\n",
      "! 5\n",
      "\" 6\n",
      "# 7\n",
      "$ 8\n",
      "% 9\n",
      "& 10\n",
      "\n",
      "Merges:\n",
      "#version: 0.2 - Trained by `huggingface/tokenizers`\n",
      "Ġ Ġ\n",
      "Ġ e\n",
      "Ġ t\n",
      "Ġ n\n",
      "Ġ a\n",
      "Ġ o\n",
      "Ġ i\n",
      "Ġ s\n",
      "Ġ r\n",
      "Ġ h\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Path to vocab and merges files\n",
    "vocab_path = \"models/vocab.json\"\n",
    "merges_path = \"models/merges.txt\"\n",
    "\n",
    "# Print a few lines of vocab.json\n",
    "print(\"Vocabulary:\")\n",
    "with open(vocab_path, 'r', encoding='utf-8') as vocab_file:\n",
    "    vocab = json.load(vocab_file)\n",
    "    for i, (token, idx) in enumerate(vocab.items()):\n",
    "        print(token, idx)\n",
    "        if i >= 10: break  # Print only the first 10 items\n",
    "\n",
    "print(\"\\nMerges:\")  # Print a separation line\n",
    "\n",
    "# Print a few lines of merges.txt\n",
    "with open(merges_path, 'r', encoding='utf-8') as merges_file:\n",
    "    for i, line in enumerate(merges_file):\n",
    "        print(line.strip())\n",
    "        if i >= 10: break  # Print only the first 10 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SentencePiece Tokenization</h2>\n",
    "Regularization During Training: Subword regularization introduces randomness in the tokenization process during training. Unlike conventional static tokenization where a text is always tokenized the same way, subword regularization randomly chooses from multiple valid subword segmentations for the same sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import PennTreebank\n",
    "import sentencepiece as spm\n",
    "\n",
    "train_data, valid_data, test_data = PennTreebank()\n",
    "\n",
    "with open('data/PennTreeBank.txt', 'w') as f:\n",
    "    for dataset in [train_data, valid_data, test_data]:\n",
    "        for sentence in dataset:\n",
    "            f.write(' '.join(sentence) + '\\n')\n",
    "spm.SentencePieceTrainer.Train('--input=data/PennTreeBank.txt --model_prefix=models/SentencePiecePennTree --vocab_size=67')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seqvenv",
   "language": "python",
   "name": "seq2seqvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
